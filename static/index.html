<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Genevieve â€” AI Learning Advisor</title>
  <style>
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

    body {
      font-family: 'Segoe UI', Arial, sans-serif;
      background: #0f0f1a;
      color: #e8e8f0;
      height: 100vh;
      display: flex;
      flex-direction: column;
      overflow: hidden;
    }

    header {
      display: flex;
      align-items: center;
      gap: 12px;
      padding: 12px 24px;
      background: #1a1a2e;
      border-bottom: 1px solid #2a2a4a;
      flex-shrink: 0;
    }
    #status-dot {
      width: 10px; height: 10px; border-radius: 50%;
      background: #ef4444; flex-shrink: 0; transition: background .3s;
    }
    #status-dot.connected { background: #22c55e; }
    header h1 { font-size: 1.05rem; font-weight: 700; color: #a78bfa; }
    header p  { font-size: .75rem; color: #666; margin-top: 1px; }

    main { flex: 1; display: flex; overflow: hidden; }

    /* â”€â”€ Avatar panel â”€â”€ */
    #avatar-panel {
      width: 300px; flex-shrink: 0;
      background: #10102a;
      display: flex; flex-direction: column;
      align-items: center; justify-content: center;
      padding: 20px 12px;
      border-right: 1px solid #2a2a4a;
      gap: 14px;
    }
    #avatar-ring {
      position: relative;
      width: 220px; height: 220px;
      border-radius: 50%;
      border: 3px solid #7c3aed;
      box-shadow: 0 0 28px rgba(124,58,237,.45), 0 0 60px rgba(124,58,237,.15);
      overflow: hidden;
      flex-shrink: 0;
    }
    /* Avatar canvas â€” pre-baked viseme sprite is drawn here each frame */
    #avatar-canvas {
      position: absolute; top: 0; left: 0;
      width: 220px; height: 220px; display: block;
    }
    #avatar-name { font-size: 1rem; font-weight: 700; color: #c4b5fd; letter-spacing: .06em; }
    #avatar-status {
      font-size: .75rem; color: #666; min-height: 1em; transition: color .3s;
    }
    #avatar-status.speaking  { color: #22c55e; }
    #avatar-status.thinking  { color: #f59e0b; }
    #avatar-status.listening { color: #38bdf8; }

    /* â”€â”€ Chat panel â”€â”€ */
    #chat-panel { flex: 1; display: flex; flex-direction: column; overflow: hidden; }
    #chat-log {
      flex: 1; overflow-y: auto;
      padding: 18px; display: flex; flex-direction: column; gap: 10px;
    }
    #chat-log::-webkit-scrollbar { width: 4px; }
    #chat-log::-webkit-scrollbar-thumb { background: #3a3a5a; border-radius: 4px; }

    .bubble {
      max-width: 80%; padding: 9px 13px;
      border-radius: 16px; font-size: .88rem; line-height: 1.5;
    }
    .bubble.user {
      align-self: flex-end; background: #4c1d95; color: #f0e8ff;
      border-bottom-right-radius: 4px;
    }
    .bubble.bot {
      align-self: flex-start; background: #1e1e3a; color: #e2e2f0;
      border: 1px solid #2e2e5a; border-bottom-left-radius: 4px;
    }
    .bubble .lbl { font-size: .68rem; font-weight: 700; opacity: .6; margin-bottom: 3px; letter-spacing: .05em; }

    .dots span {
      display: inline-block; width: 6px; height: 6px; border-radius: 50%;
      background: #7c3aed; margin: 0 2px; animation: bounce 1.1s infinite;
    }
    .dots span:nth-child(2) { animation-delay: .18s; }
    .dots span:nth-child(3) { animation-delay: .36s; }
    @keyframes bounce {
      0%,80%,100% { transform: translateY(0); opacity: .35; }
      40%          { transform: translateY(-7px); opacity: 1; }
    }

    .courses-section { align-self: flex-start; max-width: 90%; }
    .courses-hdr { font-size: .72rem; font-weight: 700; color: #a78bfa; margin-bottom: 8px; letter-spacing: .05em; }
    .card {
      background: #1a1a35; border: 1px solid #3a2a6a;
      border-radius: 11px; padding: 11px 13px; margin-bottom: 7px;
    }
    .card .ct { font-weight: 600; font-size: .88rem; color: #c4b5fd; margin-bottom: 3px; }
    .card .cm { font-size: .73rem; color: #777; margin-bottom: 5px; }
    .card .cr { font-size: .76rem; color: #999; font-style: italic; }
    .badge {
      display: inline-block; padding: 1px 7px; border-radius: 10px;
      font-size: .65rem; font-weight: 700; text-transform: uppercase; margin-right: 5px;
    }
    .badge.beginner     { background: #14532d; color: #86efac; }
    .badge.intermediate { background: #713f12; color: #fde68a; }
    .badge.advanced     { background: #450a0a; color: #fca5a5; }

    #input-bar {
      padding: 12px 18px; background: #14142a;
      border-top: 1px solid #2a2a4a;
      display: flex; gap: 8px; align-items: center; flex-shrink: 0;
    }
    #text-input {
      flex: 1; padding: 9px 14px;
      background: #1e1e38; border: 1px solid #3a3a60;
      border-radius: 22px; color: #e8e8f0; font-size: .88rem;
      outline: none; transition: border-color .2s;
    }
    #text-input:focus { border-color: #7c3aed; }
    #text-input::placeholder { color: #444; }
    .btn {
      width: 40px; height: 40px; border: none; border-radius: 50%;
      cursor: pointer; font-size: 1.05rem;
      display: flex; align-items: center; justify-content: center;
      transition: transform .1s, background .2s; flex-shrink: 0;
    }
    .btn:active { transform: scale(.9); }
    #send-btn { background: #7c3aed; color: #fff; }
    #send-btn:hover { background: #6d28d9; }
    #record-btn { background: #1e1e38; color: #ccc; border: 1px solid #3a3a60; }
    #record-btn:hover { background: #2a2a50; }
    #record-btn.recording {
      background: #dc2626; color: #fff; border-color: transparent;
      animation: rpulse 1s infinite;
    }
    @keyframes rpulse {
      0%,100% { box-shadow: 0 0 0 0 rgba(220,38,38,.5); }
      50%      { box-shadow: 0 0 0 8px rgba(220,38,38,0); }
    }
  </style>
</head>
<body>

<header>
  <div id="status-dot"></div>
  <div>
    <h1>Genevieve â€” AI Learning Advisor</h1>
    <p>Fully local &bull; gemma3:4b &bull; Whisper STT &bull; Edge TTS</p>
  </div>
</header>

<main>
  <div id="avatar-panel">
    <div id="avatar-ring">
      <!-- Single canvas: pre-baked viseme sprites swapped each frame -->
      <canvas id="avatar-canvas" width="220" height="220"></canvas>
      <!-- Neural talking loop: plays muted while TTS audio runs (viseme mode) -->
      <video id="talking-vid" loop muted playsinline preload="auto"
        style="position:absolute;top:0;left:0;width:220px;height:220px;
               object-fit:cover;object-position:50% 15%;
               border-radius:50%;z-index:5;display:none;">
        <source src="/static/videos/talking_loop.mp4" type="video/mp4">
      </video>
    </div>
    <div id="avatar-name">GENEVIEVE</div>
    <div id="avatar-status">Loadingâ€¦</div>
  </div>

  <div id="chat-panel">
    <div id="chat-log"></div>
    <div id="input-bar">
      <input id="text-input" type="text" placeholder="Type a messageâ€¦" autocomplete="off"/>
      <button id="send-btn"   class="btn" title="Send">&#9658;</button>
      <button id="record-btn" class="btn" title="Record voice">&#127908;</button>
    </div>
  </div>
</main>

<script>
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  AVATAR â€” VISEME SPRITE SWAP
//  Six sprites (v0.jpg â€¦ v5.jpg) are pre-baked at server startup.
//  mouthLoop cross-fades between adjacent sprites based on audio
//  amplitude so the mouth opens/closes smoothly in real time.
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
const CW = 220, CH = 220;
const avatarCanvas = document.getElementById('avatar-canvas');
const avatarCtx    = avatarCanvas.getContext('2d');

// Preload all viseme sprites (generated by server before first request)
const NUM_SPRITES = 6;
const sprites = Array.from({ length: NUM_SPRITES }, (_, i) => {
  const img = new Image();
  img.src = `/static/images/visemes/v${i}.jpg`;
  return img;
});

// Smooth mouth animation state (also written by the audio section)
let mouthTarget  = 0;
let mouthCurrent = 0;

// â”€â”€ Avatar init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function initAvatar() {
  const base = sprites[0];
  if (base.complete && base.naturalWidth > 0) {
    avatarCtx.drawImage(base, 0, 0, CW, CH);
    requestAnimationFrame(mouthLoop);
  } else {
    base.onload = () => {
      avatarCtx.drawImage(base, 0, 0, CW, CH);
      requestAnimationFrame(mouthLoop);
    };
    base.onerror = () => setStatus('', 'Sprites missing â€” restart server.');
  }
}

// â”€â”€ Mouth animation loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function mouthLoop() {
  // Asymmetric easing: snap open quickly (0.40), close more slowly (0.12).
  // Matches natural speech â€” mouth opens fast on each syllable and
  // lingers slightly before fully closing between words.
  const smoothFactor = mouthTarget > mouthCurrent ? 0.40 : 0.12;
  mouthCurrent += (mouthTarget - mouthCurrent) * smoothFactor;

  // Map 0â€“1 amplitude to sprite index 0â€“5 with linear interpolation
  const exactIdx = Math.max(0, Math.min(NUM_SPRITES - 1, mouthCurrent * (NUM_SPRITES - 1)));
  const lo   = Math.floor(exactIdx);
  const hi   = Math.min(NUM_SPRITES - 1, lo + 1);
  const frac = exactIdx - lo;

  const sLo = sprites[lo];
  const sHi = sprites[hi];

  if (sLo.complete && sLo.naturalWidth > 0) {
    // Draw base sprite
    avatarCtx.globalAlpha = 1;
    avatarCtx.drawImage(sLo, 0, 0, CW, CH);

    // Cross-fade to next sprite for sub-step smoothness
    if (frac > 0.01 && hi !== lo && sHi.complete && sHi.naturalWidth > 0) {
      avatarCtx.globalAlpha = frac;
      avatarCtx.drawImage(sHi, 0, 0, CW, CH);
      avatarCtx.globalAlpha = 1;
    }
  }

  requestAnimationFrame(mouthLoop);
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  AUDIO PLAYBACK + LIP SYNC
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
let audioCtx = null, analyser = null, activeSource = null, lipRaf = null;

// Resolved by the first user gesture so queued audio can play immediately.
let _audioUnlocked = false;
let _unlockResolvers = [];
function _audioUnlockedPromise() {
  if (_audioUnlocked) return Promise.resolve();
  return new Promise(res => _unlockResolvers.push(res));
}

// Call this synchronously inside every user-gesture handler.
// Creates the AudioContext (or resumes it) while the browser still trusts
// the call stack as a user-initiated action.
function _unlockAudio() {
  if (_audioUnlocked) return;
  if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  const p = audioCtx.state !== 'running' ? audioCtx.resume() : Promise.resolve();
  p.then(() => {
    _audioUnlocked = true;
    _unlockResolvers.forEach(r => r());
    _unlockResolvers = [];
  });
}

// Neural talking-loop video element (viseme mode only)
const talkingVid = document.getElementById('talking-vid');
const loopReady  = talkingVid && talkingVid.querySelector('source');

function _startTalkingLoop() {
  if (!loopReady) return;
  talkingVid.currentTime = 0;
  talkingVid.playbackRate = 1.0;
  talkingVid.style.display = 'block';
  talkingVid.play().catch(() => {});
}

function _stopTalkingLoop() {
  if (!loopReady) return;
  talkingVid.pause();
  talkingVid.style.display = 'none';
}

// â”€â”€ Audio queue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Each item: { bytes: Uint8Array, text: string, bubble: Element|null }
// Audio clips play to completion before the next one starts â€”
// no clip ever interrupts another.
const _audioQueue = [];
let   _queueRunning = false;

function enqueueAudio(base64Audio, text, bubble) {
  const bytes = Uint8Array.from(atob(base64Audio), c => c.charCodeAt(0));
  _audioQueue.push({ bytes, text, bubble });
  if (!_queueRunning) _drainQueue();
}

function _drainQueue() {
  if (_audioQueue.length === 0) {
    _queueRunning = false;
    mouthTarget = 0;
    _stopTalkingLoop();
    setStatus('connected', 'Listeningâ€¦');
    return;
  }
  _queueRunning = true;
  _playItem(_audioQueue.shift());
}

function _playItem({ bytes, text, bubble }) {
  _audioUnlockedPromise().then(() => {
    audioCtx.decodeAudioData(bytes.buffer.slice(0), buffer => {
      setStatus('speaking', 'Speakingâ€¦');

      if (lipRaf) { cancelAnimationFrame(lipRaf); lipRaf = null; }

      activeSource = audioCtx.createBufferSource();
      activeSource.buffer = buffer;

      analyser = audioCtx.createAnalyser();
      analyser.fftSize = 512;
      const freqData = new Uint8Array(analyser.frequencyBinCount);

      activeSource.connect(analyser);
      analyser.connect(audioCtx.destination);
      activeSource.start();

      _startTalkingLoop();

      // â”€â”€ Word streaming â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      // Reveal words one at a time, evenly spread across the clip duration.
      // This makes the text appear to scroll in sync with speech.
      const words = text ? text.trim().split(/\s+/).filter(Boolean) : [];
      let wordTimer = null;
      if (words.length > 0 && bubble) {
        const span = bubble.querySelector('.bot-text');
        const msPerWord = (buffer.duration * 1000) / words.length;
        let wi = 0;
        wordTimer = setInterval(() => {
          if (wi < words.length) {
            span.textContent += (wi > 0 ? ' ' : '') + words[wi++];
            chatLog.scrollTop = chatLog.scrollHeight;
          } else {
            clearInterval(wordTimer);
            wordTimer = null;
          }
        }, msPerWord);
      }

      // â”€â”€ Frequency analysis â†’ lip amplitude â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      const nyquist  = audioCtx.sampleRate / 2;
      const binHz    = nyquist / analyser.frequencyBinCount;
      const loIdx    = Math.max(1, Math.floor(100  / binHz));
      const hiIdx    = Math.min(analyser.frequencyBinCount - 1, Math.floor(3500 / binHz));
      const binCount = hiIdx - loIdx + 1;

      function lipLoop() {
        analyser.getByteFrequencyData(freqData);
        let sum = 0;
        for (let i = loIdx; i <= hiIdx; i++) sum += freqData[i];
        const avg = sum / binCount / 255;
        mouthTarget = Math.min(1, Math.pow(avg, 0.6) * 2.8);
        if (loopReady && !talkingVid.paused) {
          talkingVid.playbackRate = Math.max(0.5, Math.min(2.0, 0.5 + mouthTarget * 1.8));
        }
        lipRaf = requestAnimationFrame(lipLoop);
      }
      lipLoop();

      activeSource.onended = () => {
        cancelAnimationFrame(lipRaf); lipRaf = null;
        activeSource = null;
        // Flush any remaining words (covers edge-case timing drift)
        if (wordTimer) { clearInterval(wordTimer); wordTimer = null; }
        if (bubble) {
          const span = bubble.querySelector('.bot-text');
          if (span && text) span.textContent = text;
        }
        _drainQueue();
      };

    }, err => {
      console.error('Audio decode error:', err);
      // Decode failed: show full text immediately and move on
      if (bubble) {
        const span = bubble.querySelector('.bot-text');
        if (span && text) span.textContent = text;
      }
      _drainQueue();
    });
  });
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  WEBSOCKET
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
let ws;
function connect() {
  const proto = location.protocol === 'https:' ? 'wss' : 'ws';
  ws = new WebSocket(`${proto}://${location.host}/api/v1/ws`);
  ws.onopen  = () => {
    document.getElementById('status-dot').classList.add('connected');
    setStatus('connected', 'Connected');
  };
  ws.onmessage = evt => {
    const msg = JSON.parse(evt.data);
    if (msg.type === 'response') {
      removeThinking();

      if (msg.audio) {
        // â”€â”€ Viseme path: create an empty bubble, stream words during playback â”€â”€
        const bubble = appendBotStreaming();
        enqueueAudio(msg.audio, msg.text, bubble);

      } else if (msg.video) {
        // â”€â”€ MuseTalk path: full text shown immediately alongside the video â”€â”€
        appendBot(msg.text);
        const bytes = Uint8Array.from(atob(msg.video), c => c.charCodeAt(0));
        const blob  = new Blob([bytes], { type: 'video/mp4' });
        const url   = URL.createObjectURL(blob);
        const vid   = document.createElement('video');
        vid.src      = url;
        vid.autoplay = true;
        vid.style.cssText =
          'position:absolute;top:0;left:0;width:220px;height:220px;' +
          'object-fit:cover;border-radius:50%;z-index:10;';
        const ring = document.getElementById('avatar-ring');
        ring.appendChild(vid);
        setStatus('speaking', 'Speaking');
        vid.onended = () => {
          ring.removeChild(vid);
          URL.revokeObjectURL(url);
          setStatus('connected', 'Ready');
        };

      } else {
        // â”€â”€ No audio/video (decode error, silent fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        appendBot(msg.text);
      }
    }
    if (msg.type === 'recommendations') showCourses(msg.courses);
  };
  ws.onclose = () => {
    document.getElementById('status-dot').classList.remove('connected');
    setStatus('', 'Reconnectingâ€¦');
    setTimeout(connect, 2000);
  };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  UI HELPERS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function setStatus(cls, text) {
  const el = document.getElementById('avatar-status');
  el.className = cls; el.textContent = text;
}

const chatLog = document.getElementById('chat-log');

function appendUser(text) {
  const d = document.createElement('div');
  d.className = 'bubble user';
  d.innerHTML = `<div class="lbl">YOU</div>${esc(text)}`;
  chatLog.appendChild(d); chatLog.scrollTop = chatLog.scrollHeight;
}
function appendBot(text) {
  const d = document.createElement('div');
  d.className = 'bubble bot';
  d.innerHTML = `<div class="lbl">GENEVIEVE</div>${esc(text)}`;
  chatLog.appendChild(d); chatLog.scrollTop = chatLog.scrollHeight;
}
// Creates an empty bot bubble for word-streaming playback.
// Returns the element so _playItem can fill in words during audio.
function appendBotStreaming() {
  const d = document.createElement('div');
  d.className = 'bubble bot';
  d.innerHTML = `<div class="lbl">GENEVIEVE</div><span class="bot-text"></span>`;
  chatLog.appendChild(d); chatLog.scrollTop = chatLog.scrollHeight;
  return d;
}
function showThinking() {
  const d = document.createElement('div');
  d.className = 'bubble bot'; d.id = 'thinking';
  d.innerHTML = `<div class="lbl">GENEVIEVE</div>
    <span class="dots"><span></span><span></span><span></span></span>`;
  chatLog.appendChild(d); chatLog.scrollTop = chatLog.scrollHeight;
  setStatus('thinking', 'Thinkingâ€¦');
}
function removeThinking() { document.getElementById('thinking')?.remove(); }

function showCourses(courses) {
  const sec = document.createElement('div');
  sec.className = 'courses-section';
  sec.innerHTML = `<div class="courses-hdr">RECOMMENDED COURSES</div>`;
  courses.forEach(c => {
    const card = document.createElement('div');
    card.className = 'card';
    card.innerHTML = `
      <div class="ct">${esc(c.title)}</div>
      <div class="cm">
        <span class="badge ${c.level||''}">${c.level||''}</span>
        ${esc(c.provider)} &bull; ${esc(c.duration)} &bull; &#9733; ${c.rating}
      </div>
      <div class="cr">${esc(c.reason||'')}</div>`;
    sec.appendChild(card);
  });
  chatLog.appendChild(sec); chatLog.scrollTop = chatLog.scrollHeight;
}

function esc(s) {
  return String(s??'')
    .replace(/&/g,'&amp;').replace(/</g,'&lt;').replace(/>/g,'&gt;');
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  TEXT INPUT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function sendText() {
  const inp = document.getElementById('text-input');
  const text = inp.value.trim();
  if (!text || !ws || ws.readyState !== WebSocket.OPEN) return;
  _unlockAudio();   // unlock AudioContext synchronously inside this gesture
  appendUser(text);
  ws.send(JSON.stringify({ type: 'text', text }));
  inp.value = '';
  showThinking();
}
document.getElementById('send-btn').addEventListener('click', sendText);
document.getElementById('text-input').addEventListener('keydown', e => {
  if (e.key === 'Enter' && !e.shiftKey) { e.preventDefault(); sendText(); }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  MIC RECORDING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
let mediaRecorder = null, audioChunks = [], isRecording = false;
const recBtn = document.getElementById('record-btn');

// â”€â”€ VAD â€” Voice Activity Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// After the user starts recording, a separate AnalyserNode monitors the mic
// energy in the speech frequency band (100â€“3500 Hz) â€” the same band the
// lip-sync uses â€” so the threshold is reliable regardless of mic gain.
//
// WHY speech-band only:
//   Full-spectrum averaging spreads speech energy across ~256 bins but
//   only ~35 carry speech. Even loud talking averages to ~15/255 over
//   all bins â€” below any reasonable threshold. Averaging only the
//   speech-band bins gives 40-120 for speech vs 5-15 for silence:
//   well-separated and stable.
//
// The existing TTS analyser is completely unaffected: _vadAnalyser
// connects to the mic MediaStreamSource only â€” NOT to audioCtx.destination.
//
// State machine:
//   idle     â†’ mic open, no speech yet (never auto-stops without speech)
//   speaking â†’ speech-band energy â‰¥ VAD_SPEECH_THRESHOLD
//   silence  â†’ energy falls after speech, 1.5 s countdown starts;
//              resets immediately if user speaks again
//   done     â†’ countdown expires â†’ mediaRecorder.stop() â†’ audio sent

const VAD_SPEECH_THRESHOLD = 30;    // speech-band avg (0-255) above which = speaking
const VAD_SILENCE_MS       = 1500;  // ms of sustained silence â†’ auto-send

let _vadAnalyser  = null;
let _vadInterval  = null;
let _vadLoIdx     = 0;    // speech-band low  bin (computed in _startVAD)
let _vadHiIdx     = 0;    // speech-band high bin (computed in _startVAD)
let _silenceStart = null;
let _speechSeen   = false;

function _startVAD(stream) {
  // audioCtx is guaranteed to exist â€” _unlockAudio() ran synchronously first.
  const micSrc = audioCtx.createMediaStreamSource(stream);
  _vadAnalyser = audioCtx.createAnalyser();
  _vadAnalyser.fftSize = 1024;           // 512 bins, ~43 Hz/bin @ 44100 Hz
  _vadAnalyser.smoothingTimeConstant = 0.3;
  micSrc.connect(_vadAnalyser);          // mic â†’ analyser only, NOT â†’ destination

  // Speech-band bin indices (100â€“3500 Hz), same calculation as the lip-sync.
  const nyquist = audioCtx.sampleRate / 2;
  const binHz   = nyquist / _vadAnalyser.frequencyBinCount;
  _vadLoIdx = Math.max(1, Math.floor(100  / binHz));
  _vadHiIdx = Math.min(_vadAnalyser.frequencyBinCount - 1, Math.floor(3500 / binHz));

  const buf = new Uint8Array(_vadAnalyser.frequencyBinCount);
  _speechSeen   = false;
  _silenceStart = null;

  // 200 ms startup delay: mic hardware needs a moment to open fully.
  // The first frames are near-zero even when the user is speaking.
  setTimeout(() => {
    if (!isRecording) return;   // user already stopped manually

    _vadInterval = setInterval(() => {
      if (!isRecording) { _stopVAD(); return; }

      _vadAnalyser.getByteFrequencyData(buf);

      // Average energy in the speech band only
      let sum = 0;
      for (let i = _vadLoIdx; i <= _vadHiIdx; i++) sum += buf[i];
      const avg = sum / (_vadHiIdx - _vadLoIdx + 1);

      if (avg >= VAD_SPEECH_THRESHOLD) {
        // â”€â”€ Speech detected â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        _speechSeen   = true;
        _silenceStart = null;         // reset any active silence countdown
        setStatus('listening', 'Listeningâ€¦');

      } else if (_speechSeen) {
        // â”€â”€ Silence after speech â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if (_silenceStart === null) _silenceStart = performance.now();
        const elapsed   = performance.now() - _silenceStart;
        const remaining = Math.max(0, VAD_SILENCE_MS - elapsed);

        if (remaining > 0) {
          setStatus('listening', `Auto-send in ${(remaining / 1000).toFixed(1)}sâ€¦`);
        } else {
          // â”€â”€ Auto-stop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          _stopVAD();
          if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
          }
          isRecording = false;
          recBtn.classList.remove('recording');
          recBtn.innerHTML = '&#127908;';
          recBtn.title = 'Record voice';
        }
      }
      // else: silence before any speech â†’ keep waiting, no countdown
    }, 50);
  }, 200);
}

function _stopVAD() {
  if (_vadInterval) { clearInterval(_vadInterval); _vadInterval = null; }
  _vadAnalyser  = null;
  _silenceStart = null;
  _speechSeen   = false;
}

function getBestMime() {
  const types = [
    'audio/webm;codecs=opus', 'audio/webm',
    'audio/ogg;codecs=opus',  'audio/mp4',
  ];
  return types.find(t => MediaRecorder.isTypeSupported(t)) || '';
}

recBtn.addEventListener('click', async () => {
  _unlockAudio();   // unlock AudioContext synchronously inside this gesture
  if (isRecording) {
    _stopVAD();           // cancel any pending auto-send countdown
    mediaRecorder.stop();
    isRecording = false;
    recBtn.classList.remove('recording');
    recBtn.innerHTML = '&#127908;';
    recBtn.title = 'Record voice';
    return;
  }
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
    audioChunks = [];
    const mime = getBestMime();
    const opts = mime ? { mimeType: mime } : {};
    mediaRecorder = new MediaRecorder(stream, opts);
    const usedMime = mediaRecorder.mimeType || mime || 'audio/webm';

    mediaRecorder.ondataavailable = e => { if (e.data.size > 0) audioChunks.push(e.data); };
    mediaRecorder.onstop = () => {
      stream.getTracks().forEach(t => t.stop());
      const blob   = new Blob(audioChunks, { type: usedMime });
      audioChunks  = [];
      const reader = new FileReader();
      reader.onloadend = () => {
        const b64 = reader.result.split(',')[1];
        appendUser('ğŸ¤ [Voice message]');
        ws.send(JSON.stringify({ type: 'audio', audio: b64, mime: usedMime }));
        showThinking();
        setStatus('thinking', 'Transcribingâ€¦');
      };
      reader.readAsDataURL(blob);
    };

    mediaRecorder.start(250);
    isRecording = true;
    recBtn.classList.add('recording');
    recBtn.innerHTML = '&#9632;';
    recBtn.title = 'Stop recording';
    setStatus('listening', 'Listeningâ€¦');
    _startVAD(stream);   // begin silence-detection; auto-sends when you stop talking

  } catch (err) {
    console.error('Mic error:', err);
    alert(`Microphone error: ${err.message}\n\nPlease allow microphone access and reload.`);
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  BOOT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
initAvatar();   // load v0 sprite and start mouthLoop
connect();      // open WebSocket
</script>
</body>
</html>
