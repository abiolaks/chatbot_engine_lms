# src/nlp/ollama_conversation.py
#
# Design principles:
#   • State tracking is done SERVER-SIDE (reliable regex extraction).
#     The LLM's ONLY job is to ask for missing info in natural language.
#   • Recommendations fire as soon as the server has all three pieces.
#     The recommendation intro is HARDCODED — no LLM call, no hallucination.
#   • After recommendations the session immediately resets so the user
#     can start a new search without any mode-switching logic.
#   • Sentinel JSON extraction is removed — it was a potential injection
#     vector and is redundant since all extraction is server-side.
#   • Async-first: httpx.AsyncClient so FastAPI never blocks.

import logging
import re
from datetime import datetime
from typing import Optional

import httpx

from config import Config
from src.recommendations.engine import recommend_courses

logger = logging.getLogger(__name__)


# ── Fixed strings — never generated by the LLM ───────────────────────────────

_GREETING = (
    "Hi, I'm Genevieve, your AI learning advisor. "
    "To recommend the right courses for you, I need three things: "
    "what you want to learn, your experience level — beginner, intermediate, or advanced — "
    "and your career goal. Please go ahead."
)

# Spoken the moment all three pieces are collected and recs are ready.
# Hardcoded so the LLM cannot hallucinate course names or fabricate facts.
_REC_INTRO = (
    "Your personalised course recommendations are ready — take a look below."
)

# Spoken immediately after recommendations, inviting a new search.
_POST_REC_BRIDGE = (
    "If you'd like to explore a different topic, just tell me what you want to learn next."
)


# ── System prompt ─────────────────────────────────────────────────────────────
#
# Single, strict prompt. There is no "chatting" variant — after
# recommendations the session resets and this same prompt is reused.

_SYSTEM_PROMPT = """You are Genevieve, an AI learning advisor. Your ONLY task is to collect three pieces of information from the user so a recommendation engine can find the right courses.

The three items you must collect:
  goal   — what subject or technology they want to learn (e.g. Python, web development, SQL)
  level  — their experience level: beginner, intermediate, or advanced
  career — their target job or career goal (e.g. data scientist, web developer)

Absolute rules — break none of them:
1. Speak in ONE sentence only. This is spoken aloud by text-to-speech.
2. Ask ONLY for items that are still missing. Never mention an item already provided.
3. Ask for ALL missing items in a single sentence — never split across multiple turns.
4. Do NOT discuss anything outside of collecting the three items above.
5. Do NOT give advice, explain concepts, mention tools, or elaborate on any topic.
6. Do NOT invent, mention, or recommend any course, resource, or platform name.
7. Do NOT use filler phrases: no "Great!", "Sure!", "Absolutely!", "Of course!".
8. If the user says something off-topic, respond with exactly one sentence: "I'm here to match you with the right courses — what would you like to learn?"
9. If the user asks what you can do, respond with exactly: "Tell me what you want to learn, your experience level, and your career goal."
"""


def _build_system_prompt(collected: dict) -> str:
    """
    Append context-aware state to the strict system prompt so the LLM
    knows exactly what is already known and what is still missing.
    """
    prompt = _SYSTEM_PROMPT

    known = {k: v for k, v in collected.items() if v}
    missing = [k for k in ("goal", "level", "career") if not collected.get(k)]

    if known:
        parts = []
        if collected.get("goal"):   parts.append(f"goal = {collected['goal']}")
        if collected.get("level"):  parts.append(f"level = {collected['level']}")
        if collected.get("career"): parts.append(f"career = {collected['career']}")
        prompt += f"\n\nALREADY COLLECTED — do NOT ask again: {'; '.join(parts)}."

    if missing:
        labels = {
            "goal":   "what they want to learn",
            "level":  "their experience level (beginner / intermediate / advanced)",
            "career": "their target job or career goal",
        }
        missing_str = " and ".join(labels[k] for k in missing)
        prompt += (
            f"\n\nSTILL NEEDED: {missing_str}. "
            "Ask for all missing items in ONE short sentence."
        )

    return prompt


# ── Server-side intent extraction ─────────────────────────────────────────────
# All information is extracted here using regex — the LLM is never
# trusted to output structured data.

_LEVEL_RE = {
    "beginner": re.compile(
        r"\b(beginner|new to|just start|no experience|starting out|never|"
        r"newbie|novice|zero knowledge|complete(ly)? new|first time)\b",
        re.I,
    ),
    "intermediate": re.compile(
        r"\b(intermediate|some experience|familiar|know a (bit|little)|"
        r"been using|used before|working knowledge|some background)\b",
        re.I,
    ),
    "advanced": re.compile(
        r"\b(advanced|expert|experienced|senior|professional|proficient|"
        r"years of|deep knowledge|strong background)\b",
        re.I,
    ),
}

_GOAL_PATTERNS = [
    (re.compile(r"\b(python)\b", re.I),                                          "Python"),
    (re.compile(r"\b(machine learning|deep learning|\bml\b|\bai\b|neural|nlp)\b", re.I), "machine learning"),
    (re.compile(r"\b(web dev(elopment)?|html|css|javascript|react|node\.?js|vue|angular|frontend|front.end)\b", re.I), "web development"),
    (re.compile(r"\b(data science|data analy(sis|tics))\b", re.I),               "data science"),
    (re.compile(r"\b(sql|database|postgres|mysql|mongodb)\b", re.I),             "SQL"),
    (re.compile(r"\b(devops|docker|kubernetes|k8s|ci.?cd|cloud)\b", re.I),       "DevOps"),
    (re.compile(r"\b(java\b|spring boot)\b", re.I),                              "Java"),
    (re.compile(r"\b(excel|spreadsheet|vba)\b", re.I),                           "Excel"),
    (re.compile(r"\b(data engineer(ing)?)\b", re.I),                             "data engineering"),
    (re.compile(r"\b(cyber.?security|ethical hack(ing)?)\b", re.I),              "cybersecurity"),
]

_CAREER_PATTERNS = [
    (re.compile(r"\b(data scientist)\b", re.I),                        "data scientist"),
    (re.compile(r"\b(data analyst)\b", re.I),                          "data analyst"),
    (re.compile(r"\b(data engineer)\b", re.I),                         "data engineer"),
    (re.compile(r"\b(software engineer|software developer)\b", re.I),  "software engineer"),
    (re.compile(r"\b(web developer|full.?stack|back.?end)\b", re.I),   "web developer"),
    (re.compile(r"\b(frontend developer|front.end)\b", re.I),          "frontend developer"),
    (re.compile(r"\b(devops engineer|sre|cloud engineer)\b", re.I),    "DevOps engineer"),
    (re.compile(r"\b(product manager|\bpm\b)\b", re.I),                "product manager"),
    (re.compile(r"\b(business analyst)\b", re.I),                      "business analyst"),
    (re.compile(r"\b(ml engineer|machine learning engineer)\b", re.I), "ML engineer"),
]


def _extract_info(text: str) -> dict:
    """
    Extract goal / level / career from a single user message using regex.
    Returns only the keys that were found (never overwrites with None).
    """
    found: dict = {}

    for label, pattern in _LEVEL_RE.items():
        if pattern.search(text):
            found["level"] = label
            break

    for pattern, goal in _GOAL_PATTERNS:
        if pattern.search(text):
            found["goal"] = goal
            break

    for pattern, career in _CAREER_PATTERNS:
        if pattern.search(text):
            found["career"] = career
            break

    return found


# ── Conversation manager ───────────────────────────────────────────────────────

class OllamaConversationManager:
    """
    Manages per-session conversation state and communicates with Ollama.

    Pipeline (strict):
      1.  Greeting    — hardcoded, no LLM
      2.  Collecting  — LLM asks for missing info; server extracts values
      3.  Recommend   — hardcoded intro + recommendation engine; session resets
      4.  (back to 2) — user can immediately start a new search
    """

    def __init__(self):
        self.sessions: dict = {}
        self.base_url = Config.OLLAMA_BASE_URL
        self.model    = Config.OLLAMA_MODEL
        self.timeout  = Config.OLLAMA_TIMEOUT

    # ── Public API ────────────────────────────────────────────────────────────

    def new_session(self, session_id: str) -> dict:
        self.sessions[session_id] = {
            "session_id": session_id,
            "messages":   [],
            "collected":  {"goal": None, "level": None, "career": None},
            "created_at": datetime.now().isoformat(),
        }
        logger.info(f"New session: {session_id}")
        return self.sessions[session_id]

    async def process_message(self, session_id: str, user_text: str) -> dict:
        """
        Process one user turn.

        Returns:
          {
            "text":            str   — spoken response
            "action":          "continue" | "recommend"
            "collected_info":  dict
            "recommendations": list  — populated only when action=="recommend"
          }
        """
        if session_id not in self.sessions:
            self.new_session(session_id)

        session   = self.sessions[session_id]
        collected = session["collected"]

        # ── Step 1: Hardcoded greeting — zero LLM risk ────────────────────
        if user_text == "__init__":
            return {
                "text":            _GREETING,
                "action":          "continue",
                "collected_info":  collected,
                "recommendations": [],
            }

        # ── Step 2: Server-side extraction ────────────────────────────────
        # Extract goal / level / career from what the user just said.
        # Only store a value if it is not already known (never overwrite).
        extracted = _extract_info(user_text)
        for key, val in extracted.items():
            if val and not collected.get(key):
                collected[key] = val
                logger.info(f"[{session_id}] Extracted {key}={val!r}")

        # Add user turn to history (used as context for the LLM)
        session["messages"].append({"role": "user", "content": user_text})
        if len(session["messages"]) > Config.MAX_HISTORY:
            session["messages"] = session["messages"][-Config.MAX_HISTORY:]

        # ── Step 3: All three collected → recommend immediately ───────────
        if all(collected.get(k) for k in ("goal", "level", "career")):
            recs = recommend_courses(
                goal   = collected["goal"],
                level  = collected["level"],
                career = collected["career"],
            )
            triggered_by = dict(collected)
            logger.info(
                f"[{session_id}] Recommending for "
                f"goal={triggered_by['goal']!r} "
                f"level={triggered_by['level']!r} "
                f"career={triggered_by['career']!r}"
            )

            # Reset state so next message starts fresh collection
            session["collected"] = {"goal": None, "level": None, "career": None}
            session["messages"].append({"role": "assistant", "content": _REC_INTRO})

            return {
                "text":            _REC_INTRO,
                "action":          "recommend",
                "collected_info":  triggered_by,
                "recommendations": recs,
            }

        # ── Step 4: Ask for missing info via LLM ──────────────────────────
        system = _build_system_prompt(collected)
        reply  = await self._call_ollama(session["messages"], system=system)
        logger.debug(f"[{session_id}] Ollama: {reply!r}")

        session["messages"].append({"role": "assistant", "content": reply})

        return {
            "text":            reply,
            "action":          "continue",
            "collected_info":  collected,
            "recommendations": [],
        }

    def get_session(self, session_id: str) -> Optional[dict]:
        return self.sessions.get(session_id)

    def delete_session(self, session_id: str) -> bool:
        if session_id in self.sessions:
            del self.sessions[session_id]
            return True
        return False

    # ── Private ───────────────────────────────────────────────────────────────

    async def _call_ollama(self, messages: list, system: str = _SYSTEM_PROMPT) -> str:
        payload = {
            "model":    self.model,
            "messages": [{"role": "system", "content": system}] + messages,
            "stream":   False,
            "options": {
                "temperature": 0.2,    # Very low — maximise instruction-following
                "num_predict": 80,     # Hard cap: one sentence is ~15-30 tokens
                "stop":        ["\n", ".", "?", "!"],  # Stop at end of first sentence
            },
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                resp = await client.post(f"{self.base_url}/api/chat", json=payload)
                resp.raise_for_status()
                raw = resp.json()["message"]["content"].strip()
                # Ensure the response ends with sentence-final punctuation
                if raw and raw[-1] not in ".?!":
                    raw += "."
                return raw
        except httpx.ConnectError:
            logger.error("Ollama not reachable — is `ollama serve` running?")
            return "I'm having a little trouble — please try again in a moment."
        except Exception as exc:
            logger.exception(f"Ollama call failed: {exc}")
            return "Something went wrong. Please try again."
