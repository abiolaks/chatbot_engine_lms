# src/nlp/ollama_conversation.py
#
# Design principles:
#   • State tracking is done SERVER-SIDE (reliable regex extraction).
#     The LLM's only job is to generate a natural, short spoken reply.
#   • Recommendations fire as soon as the server has all three pieces —
#     no dependence on the model outputting a specific JSON sentinel.
#   • Dynamic system prompt tells the model exactly what is already known
#     so it never re-asks for information the user already gave.
#   • After recommendations are sent, the session enters a "chatting" phase
#     so the user can ask follow-up questions or start a new topic.
#   • Async-first: httpx.AsyncClient so FastAPI never blocks.

import json
import logging
import re
from datetime import datetime
from typing import Optional

import httpx

from config import Config
from src.recommendations.engine import recommend_courses

logger = logging.getLogger(__name__)

# ── Fixed greeting — always spoken verbatim, never generated by the LLM ──────
_GREETING = (
    "Hi, I'm Genevieve, your AI learning advisor. "
    "To recommend the right courses for you, I need three things: "
    "what you want to learn, your experience level — beginner, intermediate, or advanced — "
    "and your career goal. Please go ahead."
)

# ── Base system prompt (role + hard rules) ────────────────────────────────────
_BASE_PROMPT = """You are Genevieve, a professional AI learning advisor at an online learning platform.

Tone: professional, clear, and helpful — like a knowledgeable career counsellor. Never use filler words like "Fantastic!", "Absolutely!", "Great!", or "Sure!". Start replies directly with substance.

Hard rules:
- Every reply must be 1-2 SHORT sentences only (it is spoken aloud by TTS).
- Never re-ask for information already provided.
- Infer from context: "I'm a beginner Python developer" → goal=Python, level=beginner, career=software developer.
- Ask for ALL still-missing pieces in ONE concise question, not one at a time.
- Never say "JSON", "sentinel", or any technical term to the user.
- NEVER invent, guess, or fabricate course names, facts, or information. Only discuss what the user has told you.
- NEVER respond to topics unrelated to learning, courses, or career development. Politely redirect.
- Your ONLY job is to collect goal, level, and career — then confirm recommendations are on their way.
"""

# ── Post-recommendation conversation prompt ────────────────────────────────────
_CHATTING_PROMPT = """You are Genevieve, a professional AI learning advisor.

Tone: professional, clear, and helpful. No filler words.

You have already recommended courses in this session. You are now in a follow-up conversation.

Rules:
- Reply in 1-2 SHORT sentences only (spoken aloud by TTS).
- If the user asks about the recommended courses, their career path, or learning advice, answer directly and helpfully.
- If the user mentions a new topic they want to learn, acknowledge it briefly and ask for their experience level and career goal in ONE sentence.
- Do NOT ask for goal, level, or career if the user is asking a general follow-up question about previous recommendations.
- Be professional and substantive. No filler words.
"""


def _build_system_prompt(collected: dict, phase: str = "collecting") -> str:
    """Return a dynamic system prompt based on the current phase and collection state."""
    if phase == "chatting":
        return _CHATTING_PROMPT

    # ── collecting phase ──────────────────────────────────────────────────────
    known   = {k: v for k, v in collected.items() if v}
    missing = [k for k in ("goal", "level", "career") if not collected.get(k)]

    prompt = _BASE_PROMPT

    if known:
        desc = []
        if collected.get("goal"):   desc.append(f"learning goal = {collected['goal']}")
        if collected.get("level"):  desc.append(f"level = {collected['level']}")
        if collected.get("career"): desc.append(f"career = {collected['career']}")
        prompt += f"\n\nALREADY KNOWN — do NOT ask again: {'; '.join(desc)}."

    if missing:
        missing_labels = {
            "goal":   "what they want to learn",
            "level":  "their experience level (beginner / intermediate / advanced)",
            "career": "their target job or career goal",
        }
        missing_str = " and ".join(missing_labels[k] for k in missing)
        prompt += (
            f"\n\nSTILL MISSING: {missing_str}. "
            "Ask for ALL missing pieces in ONE short, professional sentence. "
            "Do NOT link the career question to any specific technology."
        )
    else:
        prompt += (
            "\n\nYou now have all three pieces. "
            "Reply with exactly ONE professional sentence telling the user you're finding their courses."
        )

    return prompt


# ── Server-side intent extraction ─────────────────────────────────────────────

_LEVEL_RE = {
    "beginner": re.compile(
        r"\b(beginner|new to|just start|no experience|starting out|never|newbie|novice|zero knowledge)\b",
        re.I,
    ),
    "intermediate": re.compile(
        r"\b(intermediate|some experience|familiar|know a (bit|little)|been using|used before)\b",
        re.I,
    ),
    "advanced": re.compile(
        r"\b(advanced|expert|experienced|senior|professional|proficient|years of)\b",
        re.I,
    ),
}

_GOAL_PATTERNS = [
    (re.compile(r"\b(python)\b", re.I),                              "Python"),
    (re.compile(r"\b(machine learning|deep learning|\bml\b|\bai\b|neural|nlp)\b", re.I), "machine learning"),
    (re.compile(r"\b(web dev|web development|html|css|javascript|react|node\.?js|vue|angular)\b", re.I), "web development"),
    (re.compile(r"\b(data science|data analy)\b", re.I),             "data science"),
    (re.compile(r"\b(sql|database|postgres|mysql|mongodb)\b", re.I), "SQL"),
    (re.compile(r"\b(devops|docker|kubernetes|k8s|ci.?cd|cloud)\b", re.I), "DevOps"),
    (re.compile(r"\b(java\b|spring boot)\b", re.I),                  "Java"),
    (re.compile(r"\b(excel|spreadsheet|vba)\b", re.I),               "Excel"),
    (re.compile(r"\b(data engineer)\b", re.I),                       "data engineering"),
    (re.compile(r"\b(cyber.?security|security|ethical hack)\b", re.I), "cybersecurity"),
]

_CAREER_PATTERNS = [
    (re.compile(r"\b(data scientist)\b", re.I),                      "data scientist"),
    (re.compile(r"\b(data analyst)\b", re.I),                        "data analyst"),
    (re.compile(r"\b(data engineer)\b", re.I),                       "data engineer"),
    (re.compile(r"\b(software engineer|software developer)\b", re.I),"software engineer"),
    (re.compile(r"\b(web developer|frontend|back.?end|full.?stack)\b", re.I), "web developer"),
    (re.compile(r"\b(devops engineer|sre|cloud engineer)\b", re.I),  "DevOps engineer"),
    (re.compile(r"\b(product manager|\bpm\b)\b", re.I),              "product manager"),
    (re.compile(r"\b(business analyst)\b", re.I),                    "business analyst"),
    (re.compile(r"\b(ml engineer|machine learning engineer)\b", re.I),"ML engineer"),
]


def _extract_info(text: str) -> dict:
    """
    Heuristic extraction of goal / level / career from a single user message.
    Returns only the keys that were found.
    """
    found: dict = {}

    for label, pattern in _LEVEL_RE.items():
        if pattern.search(text):
            found["level"] = label
            break

    for pattern, goal in _GOAL_PATTERNS:
        if pattern.search(text):
            found["goal"] = goal
            break

    for pattern, career in _CAREER_PATTERNS:
        if pattern.search(text):
            found["career"] = career
            break

    return found


# ── Conversation manager ───────────────────────────────────────────────────────

class OllamaConversationManager:
    """Manages per-session conversation state and communicates with Ollama."""

    def __init__(self):
        self.sessions: dict = {}
        self.base_url = Config.OLLAMA_BASE_URL
        self.model    = Config.OLLAMA_MODEL
        self.timeout  = Config.OLLAMA_TIMEOUT

    # ── Public API ────────────────────────────────────────────────────────────

    def new_session(self, session_id: str) -> dict:
        self.sessions[session_id] = {
            "session_id": session_id,
            "messages": [],
            "collected": {"goal": None, "level": None, "career": None},
            "recommendations": [],
            "phase": "collecting",   # "collecting" | "chatting"
            "created_at": datetime.now().isoformat(),
        }
        logger.info(f"New session: {session_id}")
        return self.sessions[session_id]

    async def process_message(self, session_id: str, user_text: str) -> dict:
        """
        Process one user turn. Returns:
        {
          "text":            str,
          "action":          "continue" | "recommend" | "end",
          "collected_info":  dict,
          "recommendations": list
        }
        """
        if session_id not in self.sessions:
            self.new_session(session_id)

        session   = self.sessions[session_id]
        collected = session["collected"]
        phase     = session.get("phase", "collecting")
        is_init   = (user_text == "__init__")

        # ── 0. Hardcoded greeting — no LLM call, no hallucination risk ────
        if is_init:
            return {
                "text":            _GREETING,
                "action":          "continue",
                "collected_info":  collected,
                "recommendations": [],
            }

        # ── 1. Server-side extraction (every real user turn) ──────────────
        if not is_init:
            extracted = _extract_info(user_text)

            if phase == "chatting":
                # Any new intent signals a new topic — reset collected cleanly
                if extracted:
                    logger.info(f"New topic detected in chatting phase, resetting collected.")
                    session["collected"] = {"goal": None, "level": None, "career": None}
                    collected = session["collected"]
                    session["phase"] = "collecting"
                    phase = "collecting"
                    for key, val in extracted.items():
                        if val:
                            collected[key] = val
                            logger.info(f"Extracted {key}={val!r}")
                # No new intent → stay in chatting phase, answer the question as-is
            else:
                # Collecting phase: accumulate new info
                for key, val in extracted.items():
                    if val and not collected.get(key):
                        collected[key] = val
                        logger.info(f"Extracted {key}={val!r} from user message")

            session["messages"].append({"role": "user", "content": user_text})

        # Trim history
        if len(session["messages"]) > Config.MAX_HISTORY:
            session["messages"] = session["messages"][-Config.MAX_HISTORY:]

        # ── 2. All three collected? → trigger recommendations immediately ──
        all_ready = all(collected.get(k) for k in ("goal", "level", "career"))
        if all_ready:
            recs = recommend_courses(
                goal   = collected["goal"],
                level  = collected["level"],
                career = collected["career"],
            )
            session["recommendations"] = recs

            # Save collected_info before resetting for the response payload
            triggered_by = dict(collected)

            # Switch to chatting phase and reset for potential next topic
            session["phase"] = "chatting"
            session["collected"] = {"goal": None, "level": None, "career": None}

            intro_prompt = (
                f"You have all info: goal={triggered_by['goal']}, "
                f"level={triggered_by['level']}, career={triggered_by['career']}. "
                "Reply with ONE professional sentence telling the user you are presenting their personalised course recommendations."
            )
            intro = await self._call_ollama(
                messages=[{"role": "user", "content": intro_prompt}],
                system=_BASE_PROMPT,
            )
            session["messages"].append({"role": "assistant", "content": intro})

            return {
                "text":            intro,
                "action":          "recommend",
                "collected_info":  triggered_by,
                "recommendations": recs,
            }

        # ── 3. Normal conversational turn ─────────────────────────────────
        system = _build_system_prompt(collected, phase=session.get("phase", "collecting"))
        raw    = await self._call_ollama(session["messages"], system=system)
        logger.debug(f"Ollama raw: {raw!r}")

        # Belt-and-suspenders: also check if model included sentinel JSON
        sentinel = _extract_sentinel(raw)
        if sentinel and phase != "chatting":
            for k in ("goal", "level", "career"):
                if sentinel.get(k) and not collected.get(k):
                    collected[k] = sentinel[k]
            # Re-check after sentinel extraction
            if all(collected.get(k) for k in ("goal", "level", "career")):
                recs = recommend_courses(**{k: collected[k] for k in ("goal", "level", "career")})
                session["recommendations"] = recs
                triggered_by = dict(collected)
                session["phase"] = "chatting"
                session["collected"] = {"goal": None, "level": None, "career": None}
                intro_prompt = (
                    f"All info collected. Goal={triggered_by['goal']}, "
                    f"level={triggered_by['level']}, career={triggered_by['career']}. "
                    "ONE sentence: you're showing their personalised recommendations."
                )
                intro = await self._call_ollama(
                    [{"role": "user", "content": intro_prompt}], system=_BASE_PROMPT
                )
                session["messages"].append({"role": "assistant", "content": intro})
                return {
                    "text": intro, "action": "recommend",
                    "collected_info": triggered_by, "recommendations": recs,
                }

        session["messages"].append({"role": "assistant", "content": raw})
        action = "end" if _is_farewell(raw) else "continue"

        return {
            "text":            raw,
            "action":          action,
            "collected_info":  collected,
            "recommendations": [],
        }

    def get_session(self, session_id: str) -> Optional[dict]:
        return self.sessions.get(session_id)

    def delete_session(self, session_id: str) -> bool:
        if session_id in self.sessions:
            del self.sessions[session_id]
            return True
        return False

    # ── Private ───────────────────────────────────────────────────────────────

    async def _call_ollama(self, messages: list, system: str = _BASE_PROMPT) -> str:
        payload = {
            "model":   self.model,
            "messages": [{"role": "system", "content": system}] + messages,
            "stream":  False,
            "options": {
                "temperature": 0.3,   # lower = more consistent / instruction-following
                "num_predict": 120,   # hard cap keeps TTS audio short
            },
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                resp = await client.post(f"{self.base_url}/api/chat", json=payload)
                resp.raise_for_status()
                return resp.json()["message"]["content"].strip()
        except httpx.ConnectError:
            logger.error("Ollama not reachable — is `ollama serve` running?")
            return "I'm having a little trouble thinking right now — please try again in a moment."
        except Exception as exc:
            logger.exception(f"Ollama call failed: {exc}")
            return "Sorry, something went wrong. Please try again."


# ── Utility ────────────────────────────────────────────────────────────────────

_SENTINEL_RE = re.compile(r'\{[^{}]*"action"\s*:\s*"recommend"[^{}]*\}', re.DOTALL)


def _extract_sentinel(text: str) -> Optional[dict]:
    match = _SENTINEL_RE.search(text)
    if not match:
        return None
    try:
        return json.loads(match.group())
    except json.JSONDecodeError:
        return None


_FAREWELL_RE = re.compile(r"\b(bye|goodbye|see you|take care)\b", re.I)


def _is_farewell(text: str) -> bool:
    return bool(_FAREWELL_RE.search(text))
